- CUDA - Compute Unified Device Architecture 
### 1.3 ARCHITECTURE OF A MODERN GPU
- It is organized into an array of highly threaded streaming multiprocessors(SMs).
- Two SMs form a building block; however, the number of SMs in a building block can vary from one generation of CUDA GPUs to another generation
-  SM in Figure 1.3 has a number of streaming processors (SPs) that share control logic and instruction cache.
- graphics double data rate (GDDR) DRAM, referred to as global memory. 
![[Pasted image 20250531175947.png]]

- Each SP has a multiply–add (MAD) unit and an additional multiply unit.
- With 128 SPs, that’s a total of over 500 gigaflops.
- The G80 chip supports up to 768 threads per SM, which sums up to about 12,000 threads for this chip.
- straightforward parallelization of applications often saturates the memory (DRAM) bandwidth, resulting in only about a 10 speedup. The trick is to figure out how to get around memory bandwidth limitations, which involves doing one of many transformations to utilize specialized GPU on-chip memories to drastically reduce the number of accesses to the DRAM. 
### 1.4 PARALLEL PROGRAMMING LANGUAGES AND MODELS
- The ones that are the most widely used are the Message Passing Interface ([[MPI]]) for scalable cluster computing and [[OpenMP]] for shared-memory multiprocessor systems.
- [[MPI]] has been successful in the high-performance scientific computing domain. Applications written in MPI have been known to run successfully on cluster computing systems with more than 100,000 nodes. The amount of effort required to port an application into [[MPI]], however, can be extremely high due to lack of shared memory across computing nodes.
- [[CUDA]], on the other hand, provides shared memory for parallel execution in the GPU to address this difficulty. As for CPU and GPU communication, [[CUDA]] currently provides very limited shared memory capability between the CPU and the GPU.
- [[OpenMP]] supports shared memory, so it offers the same advantage as [[CUDA]] in programming efforts; however, it has not been able to scale beyond a couple hundred computing nodes due to thread management overheads and cache coherence hardware requirements. 
- [[OpenCL]] is a standardized programming model in that applications developed in OpenCL can run without modification on all processors that support the OpenCL language extensions and API.

### 1.5 OVERARCHING GOALS
- CUDA as a simple, small extension to C that supports heterogeneous CPU/GPU joint computing and the widely used single-program, multiple-data (SPMD) parallel programming model.
	1. identifying the part of application programs to be parallelized, 
	2. isolating the data to be used by the parallelized code by using an API function to allocate memory on the parallel computing device, 
	3. using an API function to transfer data to the parallel computing device, 
	4. developing a kernel function that will be executed by individual threads in the parallelized part, 
	5. launching a kernel function for execution by parallel threads
	6. Eventually transferring the data back to the host processor with an API function call