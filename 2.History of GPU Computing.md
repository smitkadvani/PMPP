### 2.1 EVOLUTION OF GRAPHICS PIPELINES
- The remarkable advancement of graphics hardware performance has been driven by the market demand for high-quality, real-time graphics in computer applications.
#### 2.1.1 The Era of Fixed-Function Graphics Pipelines
- Example of fixed-function graphics pipeline in early NVIDIA GeForce GPUs
![[Pasted image 20250601093754.png]]
- The GeForce graphics pipeline is designed to render triangles, so the term [[vertex]] is typically used in this case to refer to the corners of a triangle.
- [[Vertex shading, transform, and lighting]] (VS/T&L) 
- [[Frame buffer interface]] (FBI) stage in Figure 2.1 manages memory reads from and writes to the display frame buffer memory.
- For high-resolution displays, there is a very high bandwidth requirement in accessing the frame buffer. 
- Such bandwidth is achieved by two strategies.
	- One 
		- is that graphics pipelines typically use special memory designs that provide higher bandwidth than the system memories.
	- Second, 
		- the FBI simultaneously manages multiple memory channels that connect to multiple memory banks. 
		- The combined bandwidth improvement of multiple channels and special memory structures gives the frame buffers much higher bandwidth than their contemporaneous system memories.
		- Such high memory bandwidth has continued to this day and has become a distinguishing feature of modern GPU design.
#### 2.1.2 Evolution of Programmable Real-Time Graphics
- In 2001, the NVIDIA GeForce 3 took the first step toward achieving true general shader programmability.
- The specific functions executed at a few graphics pipeline stages vary with rendering algorithms. Such variation has motivated the hardware designers to make those pipeline stages programmable. Two particular programmable stages stand out: 
	- [[vertex shader]]  
	- [[pixel shader]]
![[Pasted image 20250601095505.png]]
#### 2.1.3 Unified Graphics and Computing Processors
- The unified processor array allows dynamic partitioning of the array to vertex shading, geometry processing, and pixel processing.
- Because different rendering algorithms present wildly different loads among the three programmable stages, this unification allows the same pool of execution resources to be dynamically allocated to different pipeline stages and achieve better load balance.
- ![[Pasted image 20250601104244.png]]
#### 2.1.4 GPGPU: An Intermediate Step
- As DirectX 9-capable GPUs became available, some researchers took notice of the raw performance growth path of GPUs and began to explore the use of GPUs to solve compute-intensive science and engineering problems;
- however, DirectX 9 GPUs had been designed only to match the features required by the graphics APIs. To access the computational resources, a programmer had to cast his or her problem into native graphics operations so the computation could be launched through OpenGL or DirectX API calls. 
- To run many simultaneous instances of a compute function, for example, the computation had to be written as a pixel shader. The collection of input data had to be stored in texture images and issued to the GPU by submitting triangles (with clipping to a rectangle shape if that was what was desired). The output had to be cast as a set of pixels generated from the raster operations.
![[Pasted image 20250601111103.png]]
- The only way to write a result to memory was to emit it as a pixel color value, and configure the frame buffer operation stage to write (or blend, if desired) the result to a two-dimensional frame buffer.
- Furthermore, the only way to get a result from one pass of computation to the next was to write all parallel results to a pixel frame buffer, then use that frame buffer as a texture map input to the pixel fragment shader of the next stage of the computation.

### 2.2 GPU COMPUTING
- For the DirectXÔ 10 generation of graphics, NVIDIA had already begun work on a high-efficiency floating-point and integer processor that could run a variety of simultaneous workloads to support the logical graphics pipe line.
- To nongraphics application programmers, the Tesla GPU architecture introduced a more generic parallel programming model with a hierarchy of parallel threads, barrier synchronization, and atomic operations to dispatch and manage highly parallel computing work. 
- NVIDIA also developed the CUDA C/Cþþ compiler, libraries, and runtime software to enable programmers to readily access the new data-parallel computation model and develop applications.
#### 2.2.1 Scalable GPUs
- As 3D-capable accelerators began to appear, there was room in the market for a range of offerings. In 1998, 3dfx introduced multiboard scaling with their original [[Scan Line Interleave]] (SLI) on their Voodoo2, which held the performance crown for its time.
- CPUs are scaling to higher transistor counts by increasing the number of nearly-constant-performance cores on a die rather than simply increasing the performance of a single core. At this writing, the industry is transitioning from quad-core to hex-and oct-core CPUs. 
- Programmers are forced to find four- to eight-fold parallelism to fully utilize these processors. Many of them resort to coarse-grained parallelism strategies where different tasks of an application are performed in parallel. 
- Such applications must be rewritten often to have more parallel tasks for each successive doubling of core count
- In contrast, the highly multithreaded GPUs encourage the use of massive, fine-grained data parallelism in CUDA.
- Each doubling of GPU core count provides more hardware execution resources that exploit more of the exposed parallelism for higher performance; that is, the GPU parallel programming model for graphics and parallel computing is designed for transparent and portable scalability. A graphics program or CUDA program is written once and runs on a GPU with any number of processor cores.