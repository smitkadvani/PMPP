### 3.1 DATA PARALLELISM
- Data parallelism refers to the program property whereby many arithmetic operations can be safely performed on the data structures in a simultaneous manner.
- We illustrate the concept of data parallelism with a matrix–matrix multiplication
![[Pasted image 20250601115629.png]]

### 3.2 CUDA PROGRAM STRUCTURE
- A CUDA program consists of one or more phases that are executed on either the host (CPU) or a device such as a GPU. 
	- The phases that exhibit little or no data parallelism are implemented in host code. 
	- The phases that exhibit rich amount of data parallelism are implemented in the device code.
- A CUDA program is a unified source code encompassing both host and device code.
![[Pasted image 20250601120602.png]]
- The execution of a typical CUDA program is illustrated in Figure 3.2. 
- The execution starts with host (CPU) execution. When a kernel function is invoked, or launched, the execution is moved to a device (GPU), where a large number of threads are generated to take advantage of abundant data parallelism.
- All the threads that are generated by a kernel during an invocation are collectively called a grid. 
- When all threads of a kernel complete their execution, the corresponding grid terminates, and the execution continues on the host until another kernel is invoked.
### 3.3 A MATRIX–MATRIX MULTIPLICATION EXAMPLE
- first review how a conventional [[CPU-only matrix multiplication]] function works. 
- For C programs, the placement of a 2-dimensional matrix into this linear addressed memory is done according to the row-major convention.
![[Pasted image 20250601134738.png]]
- Assume that a programmer now wants to port the matrix multiplication function into CUDA. A straightforward way to do so is to modify the `MatrixMultiplication()` function to move the bulk of the calculation to a CUDA device.
- revised `MatrixMultiplication()` function is essentially an outsourcing agent that ships input data to a device, activates the calculation on the device, and collects the results from the device.
![[Pasted image 20250601135115.png]]
### 3.4 DEVICE MEMORIES AND DATA TRANSFER
- In order to execute a kernel on a device, the programmer needs to allocate memory on the device and transfer pertinent data from the host memory to the allocated device memory.
- After device execution, the programmer needs to transfer result data from the device memory back to the host memory and free up the device memory that is no longer needed. 
- CUDA device memory model for programmers to reason about the allocation, movement, and usage of the various memory types of a device.
![[Pasted image 20250601140318.png]]
- The function `cudaMalloc()` can be called from the host code to allocate a piece of global memory for an object.
- Example:  ![[Pasted image 20250601140947.png]]
- Once a program has allocated device global memory for the dataobjects, it can request that data be transferred from host to device. 
- This is accomplished by calling one of the CUDA API functions, `cudaMemcpy()`, for data transfer between memories.
- ![[Pasted image 20250601141220.png]]
- Revised MatrixMultiplication() function is complete in Part 1 and Part 3. Part 1 allocates device memory for Md,
- ![[Pasted image 20250601141535.png]]
### 3.5 KERNEL FUNCTIONS AND THREADING
- In CUDA, a kernel function specifies the code to be executed by all threads during a parallel phase.
- CUDA programming is an instance of the well-known [[single-program, multiple-data]] (SPMD) parallel programming style. 
- [[Kernel function for matrix multiplication]]
